{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-12T06:44:42.312941Z",
     "iopub.status.busy": "2021-11-12T06:44:42.312652Z",
     "iopub.status.idle": "2021-11-12T06:44:47.909429Z",
     "shell.execute_reply": "2021-11-12T06:44:47.908614Z",
     "shell.execute_reply.started": "2021-11-12T06:44:42.312909Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This file contains the trial for the final model run - LSTM + Bidirectiional + Postprocessing- with Hyperparameter Tuning.\n",
    "For best results, run the corresponding Python file.\n",
    "The outputs have been presented and discussed in the report.\n",
    "'''\n",
    "#Import necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Bidirectional, ConvLSTM2D, Flatten, Conv1D, Attention, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:44:47.913116Z",
     "iopub.status.busy": "2021-11-12T06:44:47.912890Z",
     "iopub.status.idle": "2021-11-12T06:44:48.522120Z",
     "shell.execute_reply": "2021-11-12T06:44:48.521406Z",
     "shell.execute_reply.started": "2021-11-12T06:44:47.913083Z"
    }
   },
   "outputs": [],
   "source": [
    "tweetData = pd.read_csv('Feature-Engineered.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:44:48.523948Z",
     "iopub.status.busy": "2021-11-12T06:44:48.523479Z",
     "iopub.status.idle": "2021-11-12T06:44:48.859128Z",
     "shell.execute_reply": "2021-11-12T06:44:48.858359Z",
     "shell.execute_reply.started": "2021-11-12T06:44:48.523910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Added in to avoid formatting error\n",
    "labels = np.array(tweetData['tweettype'])\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'sadness':\n",
    "        y.append(0)\n",
    "    elif labels[i] == 'neutral':\n",
    "        y.append(1)\n",
    "    elif labels[i] == 'joy':\n",
    "        y.append(2)\n",
    "    elif labels[i] == 'love':\n",
    "        y.append(3)\n",
    "    elif labels[i] == 'enthusiasm':\n",
    "        y.append(4)\n",
    "    elif labels[i] == 'anger':\n",
    "        y.append(5)\n",
    "    elif labels[i] == 'surprise':\n",
    "        y.append(6)\n",
    "    elif labels[i] == 'relief':\n",
    "        y.append(7)\n",
    "    elif labels[i] == 'fear':\n",
    "        y.append(8)\n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y, 9, dtype=\"float32\")\n",
    "del y\n",
    "\n",
    "\n",
    "def featureEngineering(tweet):\n",
    "    # Lower case tweet\n",
    "    tweetMod = tweet.lower()\n",
    "    # Replace URLs with a space in the message\n",
    "    tweetMod = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', tweetMod)\n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    tweetMod = re.sub('\\$[a-zA-Z0-9]*', ' ', tweetMod)\n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    tweetMod = re.sub('\\@[a-zA-Z0-9]*', ' ', tweetMod)\n",
    "    # Replace everything not a letter or apostrophe with a space\n",
    "    tweetMod = re.sub('[^a-zA-Z\\']', ' ', tweetMod)\n",
    "    # Remove single letter words\n",
    "    tweetMod = ' '.join([w for w in tweetMod.split() if len(w) > 1])\n",
    "\n",
    "    return tweetMod\n",
    "\n",
    "\n",
    "# Process for all tweets\n",
    "tweetData['modTweet'] = [featureEngineering(tweet) for tweet in tweetData['tweet']]\n",
    "\n",
    "def lemmatizeTweet(tweet):\n",
    "  words = [word for word in word_tokenize(tweet) if (word.isalpha()==1)]\n",
    "  # Remove stop words\n",
    "  stop = set(stopwords.words('english'))\n",
    "  words = [word for word in words if (word not in stop)]\n",
    "  # Lemmatize words (first noun, then verb)\n",
    "  wnl = nltk.stem.WordNetLemmatizer()\n",
    "  lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "  return \" \".join(lemmatized)\n",
    "\n",
    "tweetData['lemmatizedText'] = tweetData[\"modTweet\"].apply(lambda x:lemmatizeTweet(x))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=27608, split=' ')\n",
    "tokenizer.fit_on_texts(tweetData['lemmatizedText'].values)\n",
    "X = tokenizer.texts_to_sequences(tweetData['lemmatizedText'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model Structure\n",
    "Embedding + 2 Bidirectional layers + Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:47:38.434785Z",
     "iopub.status.busy": "2021-11-12T06:47:38.434535Z",
     "iopub.status.idle": "2021-11-12T06:47:39.324329Z",
     "shell.execute_reply": "2021-11-12T06:47:39.323562Z",
     "shell.execute_reply.started": "2021-11-12T06:47:38.434756Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_dropout = Sequential()\n",
    "model_dropout.add(Embedding(input_dim = 128,output_dim = 8,input_length = X.shape[1]))\n",
    "model_dropout.add(Dropout(rate=0.5))\n",
    "model_dropout.add(Bidirectional(LSTM(units=256, kernel_initializer= 'normal', return_sequences=True, kernel_constraint=maxnorm(4))))\n",
    "model_dropout.add(Dropout(rate=0.5))\n",
    "model_dropout.add(Bidirectional(LSTM(units=128, kernel_initializer= 'normal', return_sequences=False)))\n",
    "model_dropout.add(Dense(9, activation='softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "model_dropout.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model_dropout.fit(X_train, Y_train, epochs = 50, batch_size=512, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:47:50.739792Z",
     "iopub.status.busy": "2021-11-12T06:47:50.739539Z",
     "iopub.status.idle": "2021-11-12T06:47:50.754291Z",
     "shell.execute_reply": "2021-11-12T06:47:50.753567Z",
     "shell.execute_reply.started": "2021-11-12T06:47:50.739763Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting the training accuracies\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.savefig('LSTM-Bidirectional-Final-Accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:52:11.970394Z",
     "iopub.status.busy": "2021-11-12T06:52:11.970136Z",
     "iopub.status.idle": "2021-11-12T06:59:12.786611Z",
     "shell.execute_reply": "2021-11-12T06:59:12.785926Z",
     "shell.execute_reply.started": "2021-11-12T06:52:11.970365Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting the losses\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('cross-entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.savefig('LSTM-Bidirectional-Final-Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetData = pd.read_csv('Postprocessed-Feature-Engineered.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trial after clubbing output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model_dropout = Sequential()\n",
    "model_dropout.add(Embedding(input_dim = 128,output_dim = 8,input_length = X.shape[1]))\n",
    "model_dropout.add(Dropout(rate=0.5))\n",
    "model_dropout.add(Bidirectional(LSTM(units=256, kernel_initializer= 'normal', return_sequences=True, kernel_constraint=maxnorm(4))))\n",
    "model_dropout.add(Dropout(rate=0.5))\n",
    "model_dropout.add(Bidirectional(LSTM(units=128, kernel_initializer= 'normal', return_sequences=False)))\n",
    "model_dropout.add(Dense(3, activation='softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "model_dropout.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model_dropout.fit(X_train, Y_train, epochs = 50, batch_size=512, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the accuracies\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.savefig('LSTM-Postprocessed-Accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the losses\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('cross-entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.savefig('LSTM-Postprocessed-Loss.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
